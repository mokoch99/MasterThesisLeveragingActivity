{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pair Matching\n",
    "Trying to do down/upsampling by finding close matching pairs between individuals in the depression and sample cohort, meaning the filter criteria are a close match\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import sktime.datatypes\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sktime.datasets import load_arrow_head  # univariate dataset\n",
    "from sktime.datasets import load_basic_motions  # multivariate dataset\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#depression tweets\n",
    "#dt = pd.read_csv(\"data/depression_tweets.tsv\", sep=\"\\t\", index_col=\"tweet_id\", parse_dates=[2])\n",
    "#dt[\"created_at\"] = pd.to_datetime(dt[\"created_at\"])\n",
    "#n_depression = len(dt['user_id'].unique())\n",
    "#print(n_depression)\n",
    "\n",
    "#sample tweets\n",
    "st = pd.read_csv(\"data/sample_tweets.tsv\", sep=\"\\t\", index_col=\"tweet_id\", parse_dates=[2])\n",
    "#st[\"created_at\"] = pd.to_datetime(st[\"created_at\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "    user id tweets per day\n0    uD0000              3\n1    uD0004              6\n2    uD0006              3\n3    uD0007              7\n4    uD0009              5\n..      ...            ...\n598  uD1021              4\n599  uD1022              4\n600  uD1029              6\n601  uD1030              4\n602  uD1034              9\n\n[603 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user id</th>\n      <th>tweets per day</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>uD0000</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>uD0004</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>uD0006</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>uD0007</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>uD0009</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>598</th>\n      <td>uD1021</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>599</th>\n      <td>uD1022</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>600</th>\n      <td>uD1029</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>601</th>\n      <td>uD1030</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>602</th>\n      <td>uD1034</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n<p>603 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_pairs = {}\n",
    "dt = pd.read_csv(\"data/depression_tweets.tsv\", sep=\"\\t\", index_col=\"tweet_id\", parse_dates=[2])\n",
    "dt.set_index('created_at', inplace=True)\n",
    "for user in dt.user_id.unique():\n",
    "    tmp = dt[dt['user_id'] == user]\n",
    "    tweets_per_day = tmp.groupby(tmp.index.date).count()\n",
    "    n_days = len(tweets_per_day.index.unique())\n",
    "    average_pairs.update({''+user+'': ''+str(round(tweets_per_day.user_id.sum(axis=0)/n_days))+''})\n",
    "\n",
    "pairs_df = pd.DataFrame(average_pairs.items(), columns=['user id', 'tweets per day'])\n",
    "pairs_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 603/603 [00:23<00:00, 25.67it/s]\n"
     ]
    }
   ],
   "source": [
    "dt = pd.read_csv(\"data/depression_tweets.tsv\", sep=\"\\t\", index_col=\"tweet_id\", parse_dates=[2])\n",
    "dt.loc[:, 'created_at'] = pd.to_datetime(dt.loc[:, 'created_at'])\n",
    "dt_users = dt['user_id'].unique()\n",
    "\n",
    "#get filter values for each individual in depression cohort\n",
    "dt_stats = []\n",
    "for j in tqdm(range(0, len(dt_users))):\n",
    "    current_user = dt_users[j]\n",
    "    tmp_df = dt[dt['user_id'] == current_user]\n",
    "\n",
    "    #tweets per day\n",
    "    current_tpd = pairs_df[pairs_df['user id'] == current_user].iloc[0,1]\n",
    "\n",
    "    #time between tweets + total time\n",
    "    start = tmp_df['created_at'].min()\n",
    "    end = tmp_df['created_at'].max()\n",
    "    current_tt = (end-start).days\n",
    "    current_tbt = current_tt/len(tmp_df)\n",
    "\n",
    "    dt_stats.append({'user': current_user, 'tpd': current_tpd, 'tbt': current_tbt, 'total_time': current_tt})\n",
    "\n",
    "df_dt_stats = pd.DataFrame(dt_stats)\n",
    "df_dt_stats.to_csv('dt_stats.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7349/7349 [41:21<00:00,  2.96it/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'tmp_folder'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 23\u001B[0m\n\u001B[0;32m     20\u001B[0m     st_stats\u001B[38;5;241m.\u001B[39mappend({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m'\u001B[39m: current_user, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtpd\u001B[39m\u001B[38;5;124m'\u001B[39m: current_tpd, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtbt\u001B[39m\u001B[38;5;124m'\u001B[39m: current_tbt, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtotal_time\u001B[39m\u001B[38;5;124m'\u001B[39m: current_tt})\n\u001B[0;32m     22\u001B[0m df_st_stats \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(st_stats)\n\u001B[1;32m---> 23\u001B[0m \u001B[43mdf_st_stats\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;124;43mtmp_folder\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mst_stats.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\phaseUnlocking\\venv\\lib\\site-packages\\pandas\\core\\generic.py:3772\u001B[0m, in \u001B[0;36mNDFrame.to_csv\u001B[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[0m\n\u001B[0;32m   3761\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ABCDataFrame) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_frame()\n\u001B[0;32m   3763\u001B[0m formatter \u001B[38;5;241m=\u001B[39m DataFrameFormatter(\n\u001B[0;32m   3764\u001B[0m     frame\u001B[38;5;241m=\u001B[39mdf,\n\u001B[0;32m   3765\u001B[0m     header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3769\u001B[0m     decimal\u001B[38;5;241m=\u001B[39mdecimal,\n\u001B[0;32m   3770\u001B[0m )\n\u001B[1;32m-> 3772\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataFrameRenderer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3773\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath_or_buf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3774\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlineterminator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlineterminator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3775\u001B[0m \u001B[43m    \u001B[49m\u001B[43msep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3776\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3777\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3778\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3779\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquoting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquoting\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3780\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3781\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindex_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3782\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3783\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3784\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquotechar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquotechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3785\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdate_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdate_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3786\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdoublequote\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdoublequote\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3787\u001B[0m \u001B[43m    \u001B[49m\u001B[43mescapechar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mescapechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3788\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3789\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\phaseUnlocking\\venv\\lib\\site-packages\\pandas\\io\\formats\\format.py:1186\u001B[0m, in \u001B[0;36mDataFrameRenderer.to_csv\u001B[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[0m\n\u001B[0;32m   1165\u001B[0m     created_buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   1167\u001B[0m csv_formatter \u001B[38;5;241m=\u001B[39m CSVFormatter(\n\u001B[0;32m   1168\u001B[0m     path_or_buf\u001B[38;5;241m=\u001B[39mpath_or_buf,\n\u001B[0;32m   1169\u001B[0m     lineterminator\u001B[38;5;241m=\u001B[39mlineterminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1184\u001B[0m     formatter\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt,\n\u001B[0;32m   1185\u001B[0m )\n\u001B[1;32m-> 1186\u001B[0m \u001B[43mcsv_formatter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m created_buffer:\n\u001B[0;32m   1189\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path_or_buf, StringIO)\n",
      "File \u001B[1;32m~\\PycharmProjects\\phaseUnlocking\\venv\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:240\u001B[0m, in \u001B[0;36mCSVFormatter.save\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    236\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    237\u001B[0m \u001B[38;5;124;03mCreate the writer & save.\u001B[39;00m\n\u001B[0;32m    238\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    239\u001B[0m \u001B[38;5;66;03m# apply compression and byte/text conversion\u001B[39;00m\n\u001B[1;32m--> 240\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    241\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    242\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    243\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    244\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    245\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    246\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    247\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[0;32m    248\u001B[0m     \u001B[38;5;66;03m# Note: self.encoding is irrelevant here\u001B[39;00m\n\u001B[0;32m    249\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwriter \u001B[38;5;241m=\u001B[39m csvlib\u001B[38;5;241m.\u001B[39mwriter(\n\u001B[0;32m    250\u001B[0m         handles\u001B[38;5;241m.\u001B[39mhandle,\n\u001B[0;32m    251\u001B[0m         lineterminator\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlineterminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    256\u001B[0m         quotechar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquotechar,\n\u001B[0;32m    257\u001B[0m     )\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save()\n",
      "File \u001B[1;32m~\\PycharmProjects\\phaseUnlocking\\venv\\lib\\site-packages\\pandas\\io\\common.py:737\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    735\u001B[0m \u001B[38;5;66;03m# Only for write methods\u001B[39;00m\n\u001B[0;32m    736\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m is_path:\n\u001B[1;32m--> 737\u001B[0m     \u001B[43mcheck_parent_directory\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    739\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m compression:\n\u001B[0;32m    740\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m compression \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzstd\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    741\u001B[0m         \u001B[38;5;66;03m# compression libraries do not like an explicit text-mode\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\phaseUnlocking\\venv\\lib\\site-packages\\pandas\\io\\common.py:600\u001B[0m, in \u001B[0;36mcheck_parent_directory\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m    598\u001B[0m parent \u001B[38;5;241m=\u001B[39m Path(path)\u001B[38;5;241m.\u001B[39mparent\n\u001B[0;32m    599\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m parent\u001B[38;5;241m.\u001B[39mis_dir():\n\u001B[1;32m--> 600\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124mrf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot save file into a non-existent directory: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mOSError\u001B[0m: Cannot save file into a non-existent directory: 'tmp_folder'"
     ]
    }
   ],
   "source": [
    "st_pairs = pd.read_csv('pairs_tpd_users.csv')\n",
    "st.loc[:, 'created_at'] = pd.to_datetime(st.loc[:, 'created_at'])\n",
    "st_users = st['user_id'].unique()\n",
    "\n",
    "#get filter values for each individual in depression cohort\n",
    "st_stats = []\n",
    "for j in tqdm(range(0, len(st_users))):\n",
    "    current_user = st_users[j]\n",
    "    tmp_df = st[st['user_id'] == current_user]\n",
    "\n",
    "    #tweets per day\n",
    "    current_tpd = st_pairs[st_pairs['user id'] == current_user].iloc[0,2]\n",
    "\n",
    "    #time between tweets + total time\n",
    "    start = tmp_df['created_at'].min()\n",
    "    end = tmp_df['created_at'].max()\n",
    "    current_tt = (end-start).days\n",
    "    current_tbt = current_tt/len(tmp_df)\n",
    "\n",
    "    st_stats.append({'user': current_user, 'tpd': current_tpd, 'tbt': current_tbt, 'total_time': current_tt})\n",
    "\n",
    "df_st_stats = pd.DataFrame(st_stats)\n",
    "df_st_stats.to_csv('.\\\\tmp_folder\\st_stats.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# finding the closest pairs\n",
    "\n",
    "#df_dt_stats = pd.read_csv('dt_stats.csv')\n",
    "#df_st_stats = pd.read_csv('st_stats.csv')\n",
    "matches = []\n",
    "used_dt = []\n",
    "\n",
    "for j in tqdm(range(len(df_dt_stats))):\n",
    "    current = df_dt_stats.iloc[j]\n",
    "    current_user = current['user']\n",
    "    current_tpd = int(current['tpd'])\n",
    "    current_tbt = int(current['tbt'])\n",
    "    current_tt = int(current['total_time'])\n",
    "\n",
    "    closest_user = ''\n",
    "    distance = 1000000\n",
    "    for k in range(len(df_st_stats)):\n",
    "        st_current = df_st_stats.iloc[k]\n",
    "        st_user = st_current['user']\n",
    "        st_tpd = int(st_current['tpd'])\n",
    "        st_tbt = int(st_current['tbt'])\n",
    "        st_tt = int(st_current['total_time'])\n",
    "\n",
    "        #calculate distance\n",
    "        tmp_dis = abs(current_tpd-st_tpd) + abs(current_tbt-st_tbt) + abs(current_tt-st_tt)\n",
    "        if tmp_dis < distance and st_user not in used_dt:\n",
    "            distance = tmp_dis\n",
    "            closest_user = st_user\n",
    "            used_dt.append(st_user)\n",
    "\n",
    "    matches.append({'user 1': current_user, 'user 2': closest_user})\n",
    "\n",
    "df_pair_matching = pd.DataFrame(matches)\n",
    "df_pair_matching.to_csv('.\\\\tmp_folder\\pair_matching.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#df_pair_matching = pd.read_csv('pair_matching.csv')\n",
    "\n",
    "dt_users = df_pair_matching['user 1'].tolist()\n",
    "st_users = df_pair_matching['user 2'].tolist()\n",
    "\n",
    "data = {}\n",
    "labels = []\n",
    "for user in dt_users:\n",
    "    tmp = dt[dt['user_id'] == user]\n",
    "    tweet_list = tmp['created_at'].map(lambda datetime: int(round(datetime.timestamp()))).to_numpy()\n",
    "    data.update({''+user+'': tweet_list})\n",
    "    labels.append(1)\n",
    "\n",
    "for user in st_users:\n",
    "    tmp = st[st['user_id'] == user]\n",
    "    tweet_list = tmp['created_at'].map(lambda datetime: int(round(datetime.timestamp()))).to_numpy()\n",
    "    data.update({''+user+'': tweet_list})\n",
    "    labels.append(0)\n",
    "\n",
    "df_data = pd.DataFrame(dict([(key, pd.Series(value)) for key, value in data.items()]))\n",
    "df_labels = pd.DataFrame(labels)\n",
    "\n",
    "df_data = df_data.transpose()\n",
    "#df_data = df_data.drop(['Unnamed: 0'])\n",
    "#df_labels = df_labels.drop(columns=['Unnamed: 0'])\n",
    "df_data.index = range(0, len(df_data.index))\n",
    "df_data.sort_index()\n",
    "df_data = df_data.fillna(0)\n",
    "y_train, y_test, X_train, X_test = temporal_train_test_split(df_labels,df_data, test_size=0.2)\n",
    "\n",
    "#run ml and safe result with filters\n",
    "#rocket = Rocket()\n",
    "#rocket.fit(X_train)\n",
    "##X_train_transform = rocket.transform(X_train)\n",
    "#classifier = RidgeClassifierCV(alphas=np.logspace(-3,3,10))\n",
    "#classifier.fit(X_train, y_train)\n",
    "##print(X_test)\n",
    "##X_test_transform = rocket.transform(X_test)\n",
    "#X_test = X_test.replace(-float('inf'), -sys.float_info.min)\n",
    "#score = classifier.score(X_test, y_test)\n",
    "\n",
    "classifier_pipe = make_pipeline(StandardScaler(), SVC())\n",
    "classifier_pipe.fit(X_train, y_train)\n",
    "score = classifier_pipe.score(X_test, y_test)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5289256198347108"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
